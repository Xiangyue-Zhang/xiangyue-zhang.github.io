<!-- <div class="publications">
            {% bibliography -f papers -q @*[selected=true]* %}
          </div> -->
<div class="publications">
  <!-- <h2>Publications</h2> -->
  <ol class="bibliography">
    <!--Publication1-->
    <li>
      <!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <img class="img-fluid" src="assets/paper_img/ndem.png">
          <div class="abbr">
            <abbr class="badge">RAL 2023</abbr>
          </div>
        </div>
        <!-- <div class="col-sm-8">
        </div> -->
        <!-- Entry bib key -->
        <div id="yang2022real" class="col-sm-8">

          <!-- Title -->
          <div class="title">Real-Time Neural Dense Elevation Mapping for Urban Terrain With Uncertainty Estimations
          </div>
          <!-- Author -->
          <div class="author">Yang, Bowen,&nbsp;
            <em>Zhang, Qingwen</em>,&nbsp;Geng, Ruoyu,&nbsp;Wang, Lujia,&nbsp;and Liu, Ming
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In </em> 2023
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2208.03467" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0 waves-effect waves-light" role="button">Bib</a>
            <a href="https://youtu.be/CXECeoqYIR4" class="btn btn-sm z-depth-0 waves-effect waves-light" role="button"
              target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://kin-zhang.github.io/ndem" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Having good knowledge of terrain information is essential for improving the performance of various
              downstream tasks on complex terrains, especially for the locomotion and navigation of legged robots. We
              present a novel framework for neural urban terrain reconstruction with uncertainty estimations. It
              generates dense robot-centric elevation maps online from sparse LiDAR observations. We design a novel
              pre-processing and point features representation approach that ensures high robustness and computational
              efficiency when integrating multiple point cloud frames. A Bayesian-GAN model then recovers the detailed
              terrain structures while simultaneously providing the pixel-wise reconstruction uncertainty. We evaluate
              the proposed pipeline through extensive simulation and real-world experiments. It demonstrates efficient
              terrain reconstruction with high quality and real-time performance on a mobile platform, which further
              benefits the downstream tasks of legged robots.</p>
          </div>
          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight">
              <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2022real</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ndem}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Bowen and Zhang, Qingwen and Geng, Ruoyu and Wang, Lujia and Liu, Ming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-Time Neural Dense Elevation Mapping for Urban Terrain With Uncertainty Estimations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{696-703}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2022.3230325}</span><span class="p">,</span>
  <span class="na">website</span> <span class="p">=</span> <span class="s">{https://kin-zhang.github.io/ndem}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://youtu.be/CXECeoqYIR4}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2208.03467}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">badge</span> <span class="p">=</span> <span class="s">{RAL 2023}</span>
<span class="p">}</span></code></pre>
            </figure>
          </div>
        </div>
      </div>
    </li>

    <!--Publication2-->
    <li>
      <!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <img class="img-fluid" src="assets/paper_img/mmfn.png">
          <div class="abbr">
            <abbr class="badge">IROS 2022</abbr>
          </div>
        </div>
        <!-- <div class="col-sm-8">
        </div> -->
        <!-- Entry bib key -->
        <div id="zhang2022mmfn" class="col-sm-8">

          <!-- Title -->
          <div class="title">MMFN: Multi-Modal-Fusion-Net for End-to-End Driving</div>
          <!-- Author -->
          <div class="author">
            <em>Zhang, Qingwen</em>,&nbsp;Tang, Mingkai,&nbsp;Geng, Ruoyu,&nbsp;Chen, Feiyi,&nbsp;Xin, Ren,&nbsp;and
            Wang, Lujia
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2022
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2207.00186" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0 waves-effect waves-light" role="button">Bib</a>
            <a href="https://github.com/Kin-Zhang/mmfn" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Inspired by the fact that humans use diverse sensory organs to perceive the world, sensors with different
              modalities are deployed in end-to-end driving to obtain the global context of the 3D scene. In previous
              works, camera and LiDAR inputs are fused through transformers for better driving performance. These inputs
              are normally further interpreted as high-level map information to assist navigation tasks. Nevertheless,
              extracting useful information from the complex map input is challenging, for redundant information may
              mislead the agent and negatively affect driving performance. We propose a novel approach to efficiently
              extract features from vectorized High-Definition (HD) maps and utilize them in the end-to-end driving
              tasks. In addition, we design a new expert to further enhance the model performance by considering
              multi-road rules. Experimental results prove that both of the proposed improvements enable our agent to
              achieve superior performance compared with other methods.</p>
          </div>
          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight">
              <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2022mmfn</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{mmfn}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMFN: Multi-Modal-Fusion-Net for End-to-End Driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Qingwen and Tang, Mingkai and Geng, Ruoyu and Chen, Feiyi and Xin, Ren and Wang, Lujia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8638--8643}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/Kin-Zhang/mmfn}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2207.00186}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">badge</span> <span class="p">=</span> <span class="s">{IROS 2022}</span>
<span class="p">}</span></code></pre>
            </figure>
          </div>
        </div>
      </div>
    </li>

    <!--Publication3-->
    <li>
      <!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <img class="img-fluid" src="assets/paper_img/IPM.png">
          <div class="abbr">
            <abbr class="badge">RAL 2022</abbr>
          </div>
        </div>
        <!-- <div class="col-sm-8">
        </div> -->
        <!-- Entry bib key -->
        <div id="chen2022ipm" class="col-sm-8">

          <!-- Title -->
          <div class="title">Efficient Speed Planning for Autonomous Driving in Dynamic Environment With Interaction
            Point Model</div>
          <!-- Author -->
          <div class="author">Yingbing, Chen,&nbsp;Ren, Xin,&nbsp;<a href="https://jchengai.github.io" target="_blank"
              rel="noopener noreferrer">Jie, Cheng</a>,&nbsp;
            <em>Qingwen, Zhang</em>,&nbsp;Xiaodong, Mei,&nbsp;<a
              href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=ming-liu-eelium" target="_blank"
              rel="noopener noreferrer">Ming, Liu</a>,&nbsp;and Lujia, Wang
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Robotics and Automation Letters</em> 2022
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.09013" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0 waves-effect waves-light" role="button">Bib</a>
            <a href="https://github.com/ChenYingbing/IPM-Planner" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Safely interacting with other traffic participants is one of the core requirements for autonomous
              driving, especially in intersections and occlusions. Most existing approaches are designed for particular
              scenarios and require significant human labor in parameter tuning to be applied to different situations.
              To solve this problem, we first propose a learning-based Interaction Point Model (IPM), which describes
              the interaction between agents with the \textitprotection time and \textitinteraction priority in a
              unified manner. We further integrate the proposed IPM into a novel planning framework, demonstrating its
              effectiveness and robustness through comprehensive simulations in highly dynamic environments.</p>
          </div>
          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight">
              <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2022ipm</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IPM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yingbing, Chen and Ren, Xin and Jie, Cheng and Qingwen, Zhang and Xiaodong, Mei and Ming, Liu and Lujia, Wang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Speed Planning for Autonomous Driving in Dynamic Environment With Interaction Point Model}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11839-11846}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2022.3207555}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/ChenYingbing/IPM-Planner}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.09013}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">badge</span> <span class="p">=</span> <span class="s">{RAL 2022}</span>
<span class="p">}</span></code></pre>
            </figure>
          </div>
        </div>
      </div>
    </li>

    <!--Publication4-->
    <li>
      <!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <img class="img-fluid" src="assets/paper_img/gpir.png">
          <div class="abbr">
            <abbr class="badge">ICRA 2022</abbr>
          </div>
        </div>
        <!-- <div class="col-sm-8">
        </div> -->
        <!-- Entry bib key -->
        <div id="Jie2022gpir" class="col-sm-8">

          <!-- Title -->
          <div class="title">Real-Time Trajectory Planning for Autonomous Driving with Gaussian Process and Incremental
            Refinement</div>
          <!-- Author -->
          <div class="author">
            <a href="https://jchengai.github.io" target="_blank" rel="noopener noreferrer">Jie,
              Cheng</a>,&nbsp;Yingbing, Chen,&nbsp;
            <em>Qingwen, Zhang</em>,&nbsp;Lu, Gan,&nbsp;and <a
              href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=ming-liu-eelium" target="_blank"
              rel="noopener noreferrer">Ming, Liu</a>

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE International Conference on Robotics and Automation (ICRA)</em> 2022
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2205.11853" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0 waves-effect waves-light" role="button">Bib</a>
            <a href="https://github.com/jchengai/gpir" class="btn btn-sm z-depth-0 waves-effect waves-light"
              role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Real-time kinodynamic trajectory planning in dynamic environments is critical yet challenging for
              autonomous driving. In this letter, we propose an efficient trajectory planning system for autonomous
              driving in complex dynamic scenarios through iterative and incremental path-speed optimization. Exploiting
              the decoupled structure of the planning problem, a path planner based on Gaussian process first generates
              a continuous arc-length parameterized path in the Frenét frame, considering static obstacle avoidance and
              curvature constraints. We theoretically prove that it is a good generalization of the well-known jerk
              optimal solution. An efficient s-t graph search method is introduced to find a speed profile along the
              generated path to deal with dynamic environments. Finally, the path and speed are optimized incrementally
              and iteratively to ensure kinodynamic feasibility. Various simulated scenarios with both static obstacles
              and dynamic agents verify the effectiveness and robustness of our proposed method. Experimental results
              show that our method can run at 20 Hz. The source code is released as an open-source package.</p>
          </div>
          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight">
              <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Jie2022gpir</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{gpir}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-Time Trajectory Planning for Autonomous Driving with {G}aussian Process and Incremental Refinement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jie, Cheng and Yingbing, Chen and Qingwen, Zhang and Lu, Gan and Ming, Liu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/jchengai/gpir}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2205.11853}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">badge</span> <span class="p">=</span> <span class="s">{ICRA 2022}</span>
<span class="p">}</span></code></pre>
            </figure>
          </div>
        </div>
      </div>
    </li>
  </ol>
</div>